{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pyspark\n",
    "#importlib.reload(myzep)\n",
    "#from myzep import *\n",
    "#exec(open(\"myzep.py\").read());\n",
    "statecodes=[\"24.1.1\",\"11.2.1\",\"12.7.1\",\"12.7.2\",\"23.1.1\",\"23.1.4\",\"11.2.2\",\"11.2.5\",\"12.2.1\",\"12.3.1\",\"21.1.5\",\"11.3.1\",\"21.2.2\",\"3.1.1\",\"30.2.2\",\"21.2.11\",\"12.1.9\",\"13.1.1\",\"14.1.1\"]\n",
    "features= [\"hour\"]+statecodes \n",
    "#features=[\"hour\"]\n",
    "date = \"20190704\"\n",
    "rawdata =getDate(date)\n",
    "data=rawdata.filter(col(\"beam\")==30).drop(\"san\",\"time\",\"gw\",\"longitude\",\"latitude\",\"beam\")\n",
    "#rf_h2o=getH2O(data,features,\"rf\")\n",
    "#h2o_investigate(rf_h2o,data,features)\n",
    "#dt_h2o=getH2O(data,features)\n",
    "#h2o_investigate(dt_h2o,data,features)\n",
    "(train,test)=getTrainTest(data)\n",
    "gb_spark=getModel(train,test,features,\"gb\") # reorder the data\n",
    "spark_investigate(gb_spark,data,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "eurekatrees --trees rf.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import to_timestamp,from_unixtime,from_utc_timestamp,to_date,col,round,least,lit,date_trunc,max,when, sum,date_add,count,avg,min,col,lit,upper,substring,hour,unix_timestamp\n",
    "from pandas import Timestamp,date_range,read_csv,DataFrame\n",
    "period_qa_partition=\"year=2018/month=9/day=[67]\"\n",
    "t = spark.read.format(\"avro\").load(\"s3://vsat-lmtd/cust_calls/\"+period_qa_partition+\"/0.avro\").filter(col(\"case_subtype__c\").isin(\"Performance\")).select(col('account_san__c').alias('san'),date_trunc(\"day\",from_utc_timestamp(\"createddate\",\"UTC\")).alias('time'),hour(from_utc_timestamp(\"createddate\",\"UTC\")).alias('hour') )\n",
    "i= spark.read.format(\"avro\").load(\"s3://vsat-lmtd/jup1_stats/terminal_info/\"+period_qa_partition+\"/*\").select(upper(col(\"device_id\")).alias(\"san\"),to_date((col(\"ingest_time\")/1000).cast(\"Timestamp\"),\"yyyy-MM-dd\").alias(\"time\"),col(\"satellite_id\").cast(\"int\").alias(\"sat\"),col(\"gw_id\").cast(\"int\").alias(\"gw\"),col(\"absolute_beam_id\").cast(\"int\").alias(\"beam\"), col(\"hw_type\").alias(\"idu\"),substring(col(\"odu_sn\"),1,4).cast(\"int\").alias(\"odu\")).filter(col(\"time\")>lit(\"2017-01-00\")).filter(col(\"time\")<lit(\"2020-00-00\"))\n",
    "data=t.join(i,[\"san\",\"time\"]).withColumn(\"label\",when(col(\"time\")==\"2018-09-06\",0).otherwise(1))\n",
    "h2o_results = evaluate_h2o_model(data.toPandas(),feature_names=[\"gw\",\"odu\",\"hour\",\"11_2_1\",\"12_7_1\",\"12_7_2\",\"23_1_1\",\"23_1_4\",\"11_2_2\",\"11_2_5\",\"12_2_1\",\"12_3_1\",\"21_1_5\",\"11_3_1\",\"21_2_2\"],target_col='label',model=H2ODecisionTree())\n",
    "#h2o_results = evaluate_h2o_model(data.toPandas(),feature_names=[\"odu\",\"gw\",\"hour\"],target_col='label',model=H2ODecisionTree())\n",
    "#h2o_results = evaluate_h2o_model(data_categorical,feature_names=get_feature_names(data_categorical, include_c=True),target_col='y',model=H2ODecisionTree())\n",
    "print_auc_mean_std(h2o_results)\n",
    "print_sorted_mean_importances(h2o_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Statecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Aggregated Data For Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Call Outage Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athena Day, GW, Statecode culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " %sql\n",
    "--with raw1 as (select ${dim}, time, sum(raw_sum) as raw_sum,sum(call_sum) as call_sum, sum(raw_cnt) as raw_cnt,sum(call_cnt) as call_cnt from raw where statecode in ${statecode} and sat=${sat} and time > to_date(\"2018-01-01\") and time!=to_date(\"2018-12-12\") and time!=to_date(\"2018-06-14\") group by time,${dim}),\n",
    "with raw1 as (select ${dim}, statecode, time, sum(raw_sum) as raw_sum,sum(call_sum) as call_sum, sum(raw_cnt) as raw_cnt,sum(call_cnt) as call_cnt from raw where statecode in ${statecode}   and sat=${sat} and time=to_date(\"${day}\") group by time,${dim},statecode),\n",
    "sub1 as (select ${dim}, time, sum(raw_sub) as raw_sub,sum(call_sub) as call_sub from sub where sat=${sat} and time=to_date(\"${day}\") group by time,${dim}),\n",
    "raw2 as (select ${dim}, time,sum(raw_sum) over (partition by ${dim} order by time rows between ${days} preceding and current row) as raw_sum,sum(raw_cnt) over (partition by ${dim} order by time rows between ${days} preceding and current row) as raw_cnt,sum(call_sum) over (partition by ${dim} order by time rows between ${days} preceding and current row) as call_sum, sum(call_cnt) over (partition by ${dim} order by time rows between ${days} preceding and current row) as call_cnt  from raw1),\n",
    "sub2 as (select ${dim}, time, avg(raw_sub) over (partition by ${dim} order by time rows between ${days} preceding and current row) as raw_sub,avg(call_sub) over (partition by ${dim} order by time rows between ${days} preceding and current row) as call_sub from sub1),\n",
    "sub3 as (select ${dim}, rank(${mtr2}) over (order by ${mtr2} desc) as rank, ${mtr2} as max, sum(${mtr2}) over () as sum, count(*) over () as dimcnt  from sub1 ),\n",
    "raw3 as (select statecode, ${dim}, rank(${mtr1}) over (partition by statecode order by ${mtr1} desc) as rank, ${mtr1} as max, sum(${mtr1}) over (partition by statecode) as sum from raw1)\n",
    "select raw.rank, sub.max as sub_max, sub.max/sub.sum*sub.dimcnt as call_skew, raw.${dim}, raw.statecode,  raw.max/raw.sum*sub.dimcnt as sc_skew, raw.max as raw_max,raw.sum as raw_sum, sub.sum as sub_sum from raw3 raw join sub3 sub on raw.${dim}=sub.${dim} where  raw.max*sub.dimcnt>raw.sum and raw.sum>sub.max and sub.rank=1  order by raw_max desc,raw.max/raw.sum desc\n",
    "--select statecode, ${dim}, max/sum from sub3 where rank=1\n",
    "\n",
    "--select time, max(${mtr4})/sum(${mtr4}) from sub1 group by time order by time\n",
    "--select sub.time, sub.${dim}, ${metric} from raw1 raw right join sub1 sub on raw.time=sub.time and raw.${dim}=sub.${dim} order by sub.time\n",
    "--select max(raw_sub),sum(raw_sub),max(raw_sub)/sum(raw_sub) from sub1 where time=to_date(\"2018-12-29\") group by time\n",
    "--select ${dim},raw_sub from sub1 where time=to_date(\"2018-02-10\") order by raw_sub desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RF Tress Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Hour"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
