{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Timestamp\n",
    "from pyspark.sql.functions import to_timestamp,from_unixtime,from_utc_timestamp,col\n",
    "from ts.flint import FlintContext\n",
    "from pandas import date_range,read_csv,DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "flintContext = FlintContext(sqlContext)\n",
    "#df1 = tarfea.toPandas()\n",
    "#df2 = df1.resample(freq).sum()/1\n",
    "#df = tarfea.fillna(0).toPandas().set_index('time',inplace=True)\n",
    "period = 'quick'\n",
    "source ='sdg'\n",
    "if (period=='full'):\n",
    "    start_training = Timestamp(\"2019-02-01 00:00:00\", freq='H')\n",
    "    end_training = Timestamp(\"2019-02-15 00:00:00\", freq='H')\n",
    "    start_testing = Timestamp(\"2019-02-15 00:00:00\", freq='H')\n",
    "    end_testing = Timestamp(\"2019-02-28 00:00:00\", freq='H')\n",
    "    period_sdg_partition='ymd=201902*'\n",
    "    period_qa_partition='year=2019/month=2/day=*'\n",
    "    d =spark.createDataFrame(DataFrame(date_range('2019-02-01', periods=24*28, freq='H')),['time'])\n",
    "else:\n",
    "    start_training = Timestamp(\"2019-02-01 00:00:00\", freq='H')\n",
    "    end_training = Timestamp(\"2019-02-02 00:00:00\", freq='H')\n",
    "    start_testing = Timestamp(\"2019-02-02 00:00:00\", freq='H')\n",
    "    end_testing = Timestamp(\"2019-02-03 00:00:00\", freq='H')\n",
    "    period_sdg_partition='ymd=20190201'\n",
    "    period_qa_partition='year=2019/month=2/day=1'\n",
    "    d =spark.createDataFrame(DataFrame(date_range('2019-02-01', periods=24, freq='H')),['time'])\n",
    "    #sans = ['DSS36791010','DSS32702231','DSS35502688','DSS35473329']\n",
    "statecodes =['calls','24.1.1','11.3.1','14.1.1','12.7.1','12.7.2']\n",
    "target_files=\"s3://sfdmdb/SF_CASE_VW/\"+period_sdg_partition+\"/0.avro\"\n",
    "feature_parquet_files=\"s3://qaessential/jup*_stats.vsat_stcd/\"+period_sdg_partition+\"/*\"\n",
    "if (source=='sdg'):\n",
    "    feature_avro_files=\"s3://qaessential/jup*_stats.vsat_stcd/\"+period_sdg_partition+\"/*\"\n",
    "else:\n",
    "    feature_avro_files=\"s3://qaessential/jup*_stats_vsat_stcd/\"+period_qa_partition+\"/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'JavaMember' object has no attribute 'sessionState'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 383, in __repr__\n",
      "    if not self._support_repr_html and self.sql_ctx._conf.isReplEagerEvalEnabled():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/context.py\", line 99, in _conf\n",
      "    return self.sparkSession._jsparkSession.sessionState().conf()\n",
      "AttributeError: 'JavaMember' object has no attribute 'sessionState'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.read.format(\"avro\").load(target_files).select(from_utc_timestamp(\"createddate\",\"UTC\").alias('time'),col('account_san__c').alias('san'),'case_subtype__c').filter(col(\"case_subtype__c\").isin(\"Retention\",\"Performance\",\"PS TAD\"))\n",
    "#target_count = target.groupBy(\"case_subtype__c\").count().show()\n",
    "ta = flintContext.read.dataframe(t)\n",
    "#tar = ta.filter(col(\"san\").isin(sans)).show()\n",
    "#f = spark.read.format('avro').load(feature_avro_files).select(to_timestamp(from_unixtime(\"collection_start_time\", \"yyyy-MM-dd HH\")).alias('time'),col('device_id').alias('san'),'statecode','statecodeduration')\n",
    "f = spark.read.parquet(feature_parquet_files).select(to_timestamp(from_unixtime(\"collection_start_time\", \"yyyy-MM-dd HH\")).alias('time'),col('device_id').alias('san'),'statecode','statecodeduration')\n",
    "fe= flintContext.read.dataframe(f)\n",
    "#fea = fe.filter(col(\"san\").isin(sans)).show()\n",
    "ta.cache()\n",
    "fe.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import least\n",
    "s = ta.select('san')\n",
    "ds = d.crossJoin(s).sort('time')\n",
    "h=flintContext.read.dataframe(ds)\n",
    "#tar=ta.filter(col(\"san\").isin(sans)).withColumn('statecode',lit('calls')).withColumn('statecodeduration',lit(1)).drop('case_subtype__c')\n",
    "#fea=fe.filter(col(\"san\").isin(sans) & col(\"statecode\").isin(statecodes))\n",
    "tar=ta.withColumn('statecode',lit('calls')).withColumn('statecodeduration',lit(1)).drop('case_subtype__c')\n",
    "fea=fe.filter(col(\"statecode\").isin(statecodes)).withColumn('statecodeduration',least(lit(1),col('statecodeduration')/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "tf=tar.union(fea).withColumn('time',date_trunc('hour','time'))\n",
    "tf=tf.groupBy('san','time').pivot('statecode').max('statecodeduration').sort('san','time')\n",
    "#tarfea=h.leftJoin(tf,key='san',tolerance='59m').sort('san','time')\n",
    "#piv_fea=tarfea.groupBy('san','time').pivot('statecode').max('statecodeduration').sort('san','time')\n",
    "#piv_fea=h.join(tf, (h.san==tf.san) & (h.time==tf.time),how='left')\n",
    "piv_fea=h.join(tf, ['san','time'],how='left')\n",
    "df = piv_fea.fillna(0).toPandas().set_index(['san','time'])\n",
    "df.sort_values(['san','time'],inplace=True)\n",
    "#df.columns=df.columns.str.replace('_','.')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sans = s.rdd.flatMap(lambda x: x).collect()\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_training),\n",
    "        \"target\": df.loc[san].loc[start_training:end_training - 1]['calls'].tolist(),  # We use -1, because pandas indexing includes the upper bound \n",
    "        \"cat\": san,\n",
    "        \"dynamic_feat\": [df.loc[san].loc[start_training:end_training - 1]['11.3.1'].tolist(),df.loc[san].loc[start_training:end_training - 1]['14.1.1'].tolist()] # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for san in sans \n",
    "]\n",
    "testing_data = [\n",
    "    {\n",
    "        \"start\": str(start_testing),\n",
    "        \"target\": df.loc[san].loc[start_testing:end_testing - 1]['calls'].tolist(),  # We use -1, because pandas indexing includes the upper bound\n",
    "        \"dynamic_feat\": [df.loc[san].loc[start_testing:end_testing - 1]['11.3.1'].tolist(),df.loc[san].loc[start_testing:end_testing - 1]['14.1.1'].tolist()] # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for san in sans \n",
    "]\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "import json\n",
    "import boto3\n",
    "import s3fs\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))\n",
    "write_dicts_to_file('/var/lib/zeppelin/train.json',training_data)\n",
    "write_dicts_to_file('/var/lib/zeppelin/test.json',testing_data)\n",
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{} already exists.\\nSet override to upload anyway.\\n'.format(s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)\n",
    "s3_data_path='s3://jupstats'\n",
    "copy_to_s3(\"/var/lib/zeppelin/train.json\", s3_data_path + \"/train/train.json\",True)\n",
    "copy_to_s3(\"/var/lib/zeppelin/test.json\", s3_data_path + \"/test/test.json\",True)\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")\n",
    "with s3filesystem.open(s3_data_path + \"/test/test.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
