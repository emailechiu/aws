{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-172-16-8-96\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.8, OpenJDK 64-Bit Server VM, 1.8.0_121\n",
      "Branch \n",
      "Compiled by user felixcheung on 2017-11-24T23:19:45Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!/home/ec2-user/anaconda3/envs/python3/bin/pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "packages = \"org.apache.spark:spark-avro_2.11:2.4.0\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\"--packages {0} pyspark-shell\".format(packages))\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()\n",
    "spark.sparkContext.version\n",
    "#dF = spark.read.format(\"avro\").load(\"/home/ec2-user/0.avro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must instantiate with hive and hadoop tool on EMR if need to use spark\n",
    "# Must use sagemaker classpath_jars to get access to S3\n",
    "# Must use s3a: instead of s3 to use roles instead of supplying secreat key \n",
    "# Must turn on DNS to avoid pyspark jar error\n",
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "#from sagemaker_pyspark import classpath_jars # for S3 access\n",
    "#spark=SparkSession.builder.config('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0').config(\"spark.driver.extraClassPath\", \":\".join(classpath_jars())).getOrCreate()\n",
    "#spark=SparkSession.builder.config(\"spark.driver.extraClassPath\", \":\".join(classpath_jars())).getOrCreate()\n",
    "spark=SparkSession.builder.config('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0').getOrCreate()\n",
    "#spark.conf.set('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0')\n",
    "#spark.sparkContext.getConf().getAll()\n",
    "spark.version\n",
    "#dF = spark.read.format(\"avro\").load(\"/home/ec2-user/0.avro\").show()\n",
    "#dF = spark.read.format(\"avro\").load(\"s3a://sfdmdb/SF_CASE_VW/ymd=20181101/0.avro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sagemaker_pyspark import classpath_jars # for S3 access\n",
    "spark=SparkSession.builder.config(\"spark.driver.extraClassPath\", \":\".join(classpath_jars())).config('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0').getOrCreate()\n",
    "# spark.conf.set('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0')\n",
    "#dF = spark.read.format(\"avro\").load(\"/home/ec2-user/0.avro\").show()\n",
    "dF = spark.read.format(\"avro\").load(\"s3a://sfdmdb/SF_CASE_VW/ymd=20181101/0.avro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic\n",
    "%load_ext sparkmagic.magics\n",
    "%manage_spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark add -s session_ingest -l python -u http://10.0.0.155:8998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -s session_ingest\n",
    "trainingData_info = spark.read.parquet(\"s3a://qaessential/jup2_stats.terminal_info/ymd=20181129\").select('device_id','absolute_beam_id')\n",
    "trainingData_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes --name JupyterSystemEnv --channel r r-essentials=1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_info = spark.read.parquet(\"s3a://qaessential/jup2_stats.terminal_info/ymd=20181127\").select('device_id','absolute_beam_id')\n",
    "trainingData_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = spark.read.format(\"avro\").load(\"/home/ec2-user/0.avro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sagemaker_pyspark import classpath_jars # for S3 access\n",
    "spark=SparkSession.builder.config(\"spark.driver.extraClassPath\", \":\".join(classpath_jars())).config('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0').getOrCreate()\n",
    "# spark.conf.set(\"spark.jars.packages\",\"org.apache.spark:spark-avro_2.11:2.4.0\")\n",
    "#dF = spark.read.format(\"avro\").load(\"/home/ec2-user/0.avro\").show()\n",
    "dF = spark.read.format(\"avro\").load(\"s3a://sfdmdb/SF_CASE_VW/ymd=20181101/0.avro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark=SparkSession.builder.config('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0').config(\"spark.driver.extraClassPath\", \":\".join(classpath_jars())).getOrCreate()\n",
    "dF = spark.read.format(\"avro\").load(\"/home/ec2-user/0.avro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.config('spark.jars.packages','org.apache.spark:spark-avro_2.11:2.4.0').config(\"spark.driver.extraClassPath\", \":\".join(classpath_jars())).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-01 20:41:41--  https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/sparkmagic/example_config.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.248.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.248.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1894 (1.8K) [text/plain]\n",
      "Saving to: ‘example_config.json.2’\n",
      "\n",
      "example_config.json 100%[===================>]   1.85K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-03-01 20:41:41 (325 MB/s) - ‘example_config.json.2’ saved [1894/1894]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/sparkmagic/example_config.json\n",
    "!sed s/localhost/10.0.0.75/g example_config.json > /home/ec2-user/.sparkmagic/config.json\n",
    "\n",
    "!sudo aws s3 cp s3://vsat-lmtd/flint-assembly-0.6.0-SNAPSHOT.jar /opt/flint.jar\n",
    "!sudo pip-3.6 install ts-flint\n",
    "!sudo pip-3.6 install pandas\n",
    "!sudo pip-3.6 install pyarrow\n",
    "\n",
    "!sudo sed -i -e '$a\\export PYSPARK_PYTHON=/usr/bin/python3' /etc/spark/conf/spark-env.sh\n",
    "!sudo sed -i 's/extraClassPath.*/&:\\/opt\\/flint.jar/g' /etc/spark/conf/spark-defaults.conf\n",
    "!sudo sed -i '$aspark.jars.packages              org.apache.spark:spark-avro_2.11:2.4.0' /etc/spark/conf/spark-defaults.conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
