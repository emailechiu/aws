{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function squaredWithPython at 0x7f9175025578>"
     ]
    }
   ],
   "source": [
    "def squared(s):\n",
    "  return s * s\n",
    "spark.udf.register(\"squaredWithPython\", squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function squaredWithPython at 0x7f91741a49b0>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "def squared_typed(s):\n",
    "  return s * s\n",
    "spark.udf.register(\"squaredWithPython\", squared, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1, 20).registerTempTable(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An internal error was encountered.\n",
      "Please file an issue at https://github.com/jupyter-incubator/sparkmagic\n",
      "Error:\n",
      "Cannot parse object as JSON: '['----------------------------------------', \"Exception happened during processing of request from ('127.0.0.1', 46850)\", '----------------------------------------', '{\"id\":1,\"id_squared\":1}', '{\"id\":2,\"id_squared\":4}', '{\"id\":3,\"id_squared\":9}', '{\"id\":4,\"id_squared\":16}', '{\"id\":5,\"id_squared\":25}', '{\"id\":6,\"id_squared\":36}', '{\"id\":7,\"id_squared\":49}', '{\"id\":8,\"id_squared\":64}', '{\"id\":9,\"id_squared\":81}', '{\"id\":10,\"id_squared\":100}', '{\"id\":11,\"id_squared\":121}', '{\"id\":12,\"id_squared\":144}', '{\"id\":13,\"id_squared\":169}', '{\"id\":14,\"id_squared\":196}', '{\"id\":15,\"id_squared\":225}', '{\"id\":16,\"id_squared\":256}', '{\"id\":17,\"id_squared\":289}', '{\"id\":18,\"id_squared\":324}', '{\"id\":19,\"id_squared\":361}', 'Traceback (most recent call last):', '  File \"/usr/lib64/python2.7/SocketServer.py\", line 290, in _handle_request_noblock', '    self.process_request(request, client_address)', '  File \"/usr/lib64/python2.7/SocketServer.py\", line 318, in process_request', '    self.finish_request(request, client_address)', '  File \"/usr/lib64/python2.7/SocketServer.py\", line 331, in finish_request', '    self.RequestHandlerClass(request, client_address, self)', '  File \"/usr/lib64/python2.7/SocketServer.py\", line 652, in __init__', '    self.handle()', '  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 263, in handle', '    poll(authenticate_and_accum_updates)', '  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 238, in poll', '    if func():', '  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 251, in authenticate_and_accum_updates', '    received_token = self.rfile.read(len(auth_token))', \"TypeError: object of type 'NoneType' has no len()\"]'\n"
     ]
    }
   ],
   "source": [
    "%%sql \n",
    "select id, squaredWithPython(id) as id_squared from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
